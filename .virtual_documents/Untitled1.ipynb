get_ipython().getoutput("pip install --upgrade pip")
get_ipython().getoutput("pip install torch transformers accelerate bitsandbytes peft trl datasets huggingface_hub")
get_ipython().getoutput("pip install -q einops")


get_ipython().getoutput("pip install --upgrade multiprocess dill")


get_ipython().getoutput("pip install --upgrade datasets")



get_ipython().getoutput("pip install --upgrade datasets")



from huggingface_hub import notebook_login

# Run this and enter your Hugging Face token when prompted
notebook_login()


import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset
import os


model_name = "meta-llama/Llama-3.2-3B-Instruct"

# 4-bit quantization configuration
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token  # Required for batched training

# Load pre-trained model with QLoRA (4-bit)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",  # Automatically uses GPU(s)
    trust_remote_code=True
)

# Prepare model for LoRA training
model = prepare_model_for_kbit_training(model)


peft_config = LoraConfig(
    r=64,  # Rank of the low-rank matrices
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # Common for Llama/Mistral
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

# Wrap the model with LoRA
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()


# Load dataset (example: Dolly-15k)
dataset = load_dataset("json", data_files="train2.jsonl", split="train")

# Optional: Use a smaller subset for faster testing
dataset = dataset.select(range(1000))  # Remove this line for full training

def format_instruction(example):
    return {
        "text": f"<|start_of_turn|>user\n{example['Instruction']}<|end_of_turn|>"
               f"<|start_of_turn|>model\n{example['Response']}<|end_of_turn|>"
    }

dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)


get_ipython().getoutput("pip install --upgrade transformers")


get_ipython().getoutput("pip show transformers")


output_dir = "./results"
os.makedirs(output_dir, exist_ok=True)

training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=3,
    per_device_train_batch_size=1,       # Reduce if OOM
    gradient_accumulation_steps=8,       # Effective batch size = 8
    optim="paged_adamw_8bit",            # Handle memory better
    save_steps=100,
    logging_steps=25,
    learning_rate=2e-4,
    weight_decay=0.01,
    fp16=True,
    bf16=False,  # Enable if GPU supports it (Ampere+)
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    lr_scheduler_type="cosine",
    report_to="none",  # Change to "wandb" if using logging
    save_strategy="steps",
    save_total_limit=2,
    load_best_model_at_end=False,
    eval_strategy="no",  # No eval set for simplicity
    gradient_checkpointing=True,
    run_name="q_lora_finetune_run"
)


data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # Not masked LM, causal LM
)


get_ipython().getoutput("pip install --upgrade trl peft transformers accelerate bitsandbytes")


def formatting_func(example):
    text = example["text"]
    return [text]


def tokenize_function(examples):
    return tokenizer(
        examples["text"],           # replace "text" with your column name
        truncation=True,
        max_length=512,
        padding=False,
        return_attention_mask=True
    )


tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    num_proc=2,
    remove_columns=["text"],  # Remove original text column
    desc="Tokenizing dataset"
)


get_ipython().getoutput("pip install --upgrade trl peft transformers accelerate bitsandbytes")


import trl
print("trl version:", trl.__version__)


get_ipython().getoutput("pip uninstall -y trl")


get_ipython().getoutput("pip install trl==0.21.0")


trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    peft_config=peft_config,
    formatting_func=formatting_func,  # Function to merge Instruction + Response
)


if hasattr(model, "peft_config"):
    model = model.unload()


model = get_peft_model(model, peft_config)


trainer.train()
trainer.save_model("./q_lora_adapter")
tokenizer.save_pretrained("./q_lora_adapter")


model.save_pretrained("./q_lora_adapter")
tokenizer.save_pretrained("./q_lora_adapter")
print("âœ… Adapter saved!")


def generate(model, prompt, max_new_tokens=150):
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id
    )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


# Load datasets
ds1 = load_dataset("json", data_files="train2.jsonl", split="train")
ds2 = load_dataset("json", data_files="constitution_qa.jsonl", split="train")
ds3 = load_dataset("json", data_files="ipc_qa.jsonl", split="train")
ds4 = load_dataset("json", data_files="crpc_qa.jsonl", split="train")


def standardize_example(example):
    # Handle different field names
    if "question" in example:
        instruction = example["question"]
    elif "Instruction" in example:
        instruction = example["Instruction"]
    else:
        instruction = example["instruction"]

    if "answer" in example:
        response = example["answer"]
    elif "Response" in example:
        response = example["Response"]
    else:
        response = example["response"]

    # Optional: include context if available
    context = ""
    if "context" in example and example["context"].strip():
        context = f"\n\nContext: {example['context'].strip()}"

    # Return unified format
    return {
        "instruction": instruction.strip(),
        "context": context,
        "response": response.strip()
    }

# Apply to all datasets
ds1_std = ds1.map(lambda x: standardize_example(x), remove_columns=ds1.column_names)
ds2_std = ds2.map(lambda x: standardize_example(x), remove_columns=ds2.column_names)
ds3_std = ds3.map(lambda x: standardize_example(x), remove_columns=ds3.column_names)
ds4_std = ds4.map(lambda x: standardize_example(x), remove_columns=ds4.column_names)


from datasets import concatenate_datasets

merged_dataset = concatenate_datasets([ds1_std, ds2_std, ds3_std, ds4_std])

print(f"Total training examples: {len(merged_dataset)}")


def format_instruction(example):
    return {
        "text": (
            f"### Instruction:\n{example['instruction']}"
            f"{example['context']}\n\n"
            f"### Response:\n{example['response']}"
        )
    }

# Apply formatting
merged_dataset = merged_dataset.map(format_instruction, remove_columns=merged_dataset.column_names)


# Load your previously saved adapter
model = PeftModel.from_pretrained(base_model, "./q_lora_adapter")

# No need to re-wrap with get_peft_model() â€” already a PeftModel
# Just continue training
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    peft_config=peft_config,
    formatting_func=formatting_func,  # Function to merge Instruction + Response
)

# This will update the existing LoRA weights
trainer.train()


from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Your model name
model_name = "meta-llama/Llama-3.2-3B-Instruct"

# 4-bit config (same as before)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load base model (no LoRA)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)


# Now load your adapter on top
from peft import PeftModel

finetuned_model = PeftModel.from_pretrained(
    base_model, 
    "./q_lora_adapter"  # Your trained LoRA
)


def ask(model, prompt, max_new_tokens=200):
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()


questions = [
    "What is the difference between murder and culpable homicide?",
    "Explain the concept of 'basic structure doctrine' in Indian Constitution.",
    "When can a writ of habeas corpus be filed?",
    "What are the rights of an accused under Article 22?",
    "What is anticipatory bail under Section 438 of CrPC?"
]

for q in questions:
    prompt = f"### Instruction:\n{q}\n\n### Response:\n"
    
    print(f"ðŸ”¸ Question: {q}\n")
    
    print("ðŸŸ¡ Base Model:")
    print(ask(base_model, prompt))
    print("\n")
    
    print("ðŸŸ¢ Fine-Tuned Model:")
    print(ask(finetuned_model, prompt))
    print("\n" + "="*80 + "\n")



